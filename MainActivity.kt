package com.example.iris

import android.Manifest
import android.content.Intent
import android.content.pm.PackageManager
import android.os.Bundle
import android.speech.RecognitionListener
import android.speech.RecognizerIntent
import android.speech.SpeechRecognizer
import android.speech.tts.TextToSpeech
import android.util.Log
import android.widget.Toast
import androidx.activity.ComponentActivity
import androidx.activity.compose.setContent
import androidx.activity.enableEdgeToEdge
import androidx.compose.foundation.layout.*
import androidx.compose.material3.*
import androidx.compose.runtime.*
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.unit.dp
import androidx.core.app.ActivityCompat
import androidx.core.content.ContextCompat
import androidx.lifecycle.lifecycleScope
import com.example.iris.ui.WaveformVisualizer
import com.example.iris.ui.theme.IRISTheme
import kotlinx.coroutines.launch
import okhttp3.*
import okhttp3.MediaType.Companion.toMediaTypeOrNull
import org.json.JSONArray
import org.json.JSONObject
import java.io.IOException
import java.util.*
import java.util.concurrent.TimeUnit

class MainActivity : ComponentActivity() {
    private lateinit var tts: TextToSpeech
    private lateinit var client: OkHttpClient
    private val apiKey = "Bearer sk-..." // üëâ Ïó¨Í∏∞Ïóê Ïã§Ï†ú OpenAI API ÌÇ§ ÏûÖÎ†•

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        enableEdgeToEdge()

        client = OkHttpClient.Builder()
            .connectTimeout(30, TimeUnit.SECONDS)
            .readTimeout(60, TimeUnit.SECONDS)
            .writeTimeout(60, TimeUnit.SECONDS)
            .build()

        setContent {
            IRISTheme {
                val assistantReplyState = remember { mutableStateOf("ÏïÑÏù¥Î¶¨Ïä§Ïóê Ïò§Ïã† Í±∏ ÌôòÏòÅÌï©ÎãàÎã§!") }
                val isWaveformActive = remember { mutableStateOf(false) }

                IrisUI(
                    assistantReplyState = assistantReplyState,
                    isWaveformActive = isWaveformActive,
                    onVoiceInputClick = {
                        if (!checkAudioPermission()) return@IrisUI
                        startSpeechRecognition(assistantReplyState, isWaveformActive)
                    },
                    onTextSubmit = { text ->
                        assistantReplyState.value = "Ïû†ÏãúÎßå Í∏∞Îã§Î†§ Ï£ºÏÑ∏Ïöî..."
                        isWaveformActive.value = true
                        lifecycleScope.launch {
                            getChatbotResponse(text, assistantReplyState, isWaveformActive)
                        }
                    }
                )
            }
        }
    }

    @Composable
    fun IrisUI(
        assistantReplyState: MutableState<String>,
        isWaveformActive: MutableState<Boolean>,
        onVoiceInputClick: () -> Unit,
        onTextSubmit: (String) -> Unit
    ) {
        var userInput by remember { mutableStateOf("") }

        Column(
            modifier = Modifier
                .fillMaxSize()
                .padding(24.dp),
            verticalArrangement = Arrangement.Center,
            horizontalAlignment = Alignment.CenterHorizontally
        ) {
            WaveformVisualizer(isActive = isWaveformActive.value)

            Spacer(modifier = Modifier.height(16.dp))

            Text(
                text = assistantReplyState.value,
                style = MaterialTheme.typography.headlineSmall
            )

            Spacer(modifier = Modifier.height(32.dp))

            Button(onClick = onVoiceInputClick) {
                Text("ÏïÑÏù¥Î¶¨Ïä§ÏóêÍ≤å ÎßêÌïòÍ∏∞ (ÏùåÏÑ±)")
            }

            Spacer(modifier = Modifier.height(24.dp))

            OutlinedTextField(
                value = userInput,
                onValueChange = { userInput = it },
                label = { Text("ÌÖçÏä§Ìä∏Î°ú ÏïÑÏù¥Î¶¨Ïä§ÏóêÍ≤å ÎßêÌïòÍ∏∞") },
                singleLine = true,
                modifier = Modifier.fillMaxWidth()
            )

            Spacer(modifier = Modifier.height(8.dp))

            Button(onClick = {
                if (userInput.isNotBlank()) {
                    onTextSubmit(userInput)
                    userInput = ""
                }
            }) {
                Text("ÌÖçÏä§Ìä∏ Ï†ÑÏÜ°")
            }
        }
    }

    private fun checkAudioPermission(): Boolean {
        val permission = Manifest.permission.RECORD_AUDIO
        return if (ContextCompat.checkSelfPermission(this, permission) != PackageManager.PERMISSION_GRANTED) {
            ActivityCompat.requestPermissions(this, arrayOf(permission), 1001)
            false
        } else {
            true
        }
    }

    private suspend fun getChatbotResponse(
        userInput: String,
        assistantReplyState: MutableState<String>,
        isWaveformActive: MutableState<Boolean>
    ) {
        val url = "https://api.openai.com/v1/chat/completions"

        val messagesArray = JSONArray()
        messagesArray.put(JSONObject().apply {
            put("role", "user")
            put("content", userInput)
        })

        val jsonBody = JSONObject()
        jsonBody.put("model", "gpt-3.5-turbo")
        jsonBody.put("messages", messagesArray)

        val requestBody = RequestBody.create(
            "application/json".toMediaTypeOrNull(),
            jsonBody.toString()
        )

        val request = Request.Builder()
            .url(url)
            .addHeader("Authorization", apiKey)
            .addHeader("Content-Type", "application/json")
            .post(requestBody)
            .build()

        client.newCall(request).enqueue(object : Callback {
            override fun onFailure(call: Call, e: IOException) {
                runOnUiThread {
                    assistantReplyState.value = "API Ìò∏Ï∂ú Ïã§Ìå®: ${e.message}"
                    isWaveformActive.value = false
                }
            }

            override fun onResponse(call: Call, response: Response) {
                isWaveformActive.value = false
                if (response.isSuccessful) {
                    val body = response.body?.string()
                    val reply = JSONObject(body!!)
                        .getJSONArray("choices")
                        .getJSONObject(0)
                        .getJSONObject("message")
                        .getString("content")

                    runOnUiThread {
                        assistantReplyState.value = reply.trim()
                        isWaveformActive.value = true
                        tts.speak(reply, TextToSpeech.QUEUE_FLUSH, null, null)
                        isWaveformActive.value = false
                    }
                } else {
                    runOnUiThread {
                        assistantReplyState.value = "API Ïò§Î•ò: ${response.code}"
                        isWaveformActive.value = false
                    }
                }
            }
        })
    }

    private fun startSpeechRecognition(
        assistantReplyState: MutableState<String>,
        isWaveformActive: MutableState<Boolean>
    ) {
        val recognizer = SpeechRecognizer.createSpeechRecognizer(this)

        val intent = Intent(RecognizerIntent.ACTION_RECOGNIZE_SPEECH).apply {
            putExtra(RecognizerIntent.EXTRA_LANGUAGE_MODEL, RecognizerIntent.LANGUAGE_MODEL_FREE_FORM)
            putExtra(RecognizerIntent.EXTRA_LANGUAGE, "ko-KR")
        }

        recognizer.setRecognitionListener(object : RecognitionListener {
            override fun onReadyForSpeech(params: Bundle?) {
                assistantReplyState.value = "ÏïÑÏù¥Î¶¨Ïä§Í∞Ä Îì£Í≥† ÏûàÏñ¥Ïöî..."
                isWaveformActive.value = true
            }

            override fun onResults(results: Bundle?) {
                isWaveformActive.value = false
                val spokenText = results?.getStringArrayList(SpeechRecognizer.RESULTS_RECOGNITION)?.get(0)
                val userText = spokenText ?: "Î¨¥Ïä® ÎßêÏù∏ÏßÄ Î™ª Îì§ÏóàÏñ¥Ïöî."
                assistantReplyState.value = userText
                tts.speak(userText, TextToSpeech.QUEUE_FLUSH, null, null)
            }

            override fun onError(error: Int) {
                assistantReplyState.value = "ÏùåÏÑ± Ïù∏Ïãù Ï§ë Ïò§Î•ò Î∞úÏÉù: $error"
                isWaveformActive.value = false
            }

            override fun onBeginningOfSpeech() {}
            override fun onEndOfSpeech() {
                isWaveformActive.value = false
            }
            override fun onRmsChanged(rmsdB: Float) {}
            override fun onBufferReceived(buffer: ByteArray?) {}
            override fun onPartialResults(partialResults: Bundle?) {}
            override fun onEvent(eventType: Int, params: Bundle?) {}
        })

        recognizer.startListening(intent)
    }

    override fun onDestroy() {
        super.onDestroy()
        tts.stop()
        tts.shutdown()
    }
}
